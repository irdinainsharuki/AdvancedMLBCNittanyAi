{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irdinainsharuki/AdvancedMLBCNittanyAi/blob/main/Project%203%20-%20RAGChatBot\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç Objective:\n",
        "\n",
        "This project will introduce you to Retrieval Augmented Generation and how it can be used to expand the knowledge base of an existing pretrained LLM.\n",
        "\n",
        "# üß†Glossary:\n",
        "\n",
        "We're going to be using some rather fancy sounding words that you may not have come across before. I'd highly reccommend googling them, but feel free to refer to this cheat sheet if you forget.\n",
        "\n",
        "\n",
        "1.   Vector: Think of this as a list of numbers specifically used to represent co-ordinates. So a vector containing the co-ordinates to a point at (2,8) would be [2, 8]. Now we'll often be using arrays of vectors, we'll also be using a library called numpy, which supports 2D arrays (basically an array of arrays) much better than regular Python.\n",
        "\n",
        "2.   Embeddings:\n",
        "\n",
        "*   A huge part of RAG is semantic search (i.e. searching by meaning). This is the core of what makes RAG so powerful, as it can tell us how similar two sentences are based on what they mean, even if they are worded differently.\n",
        "\n",
        "*   The way this is done is by using an embedding model to convert text (or images) to points in space. The closer two points are, the closer in meaning their corresponding texts are.\n",
        "\n",
        "*   We will be storing the co-ordinates of the points in vectors, hence the need for a vector database.\n",
        "\n",
        "*   Note: While it is helpful to think of the points generated by an embedding model as points in 3 dimensions, most embedding models generate points in higher dimensions; the model we are using generates points in 384 dimensions! So instead of our points having an x, y and z co-ordinate, they will have x, y, z, w, v,..... co-odinates.\n",
        "\n",
        "3. LLM: Large Language Models (LLM) can generate text based on a provided prompt. Sound familiar? It should; ChatGPT is a Large Language Model! We'll be using an LLM to actually answer a users question. The problem here is that LLMs aren't all knowing; they can only answer questions based on what they've been trained on. A way to remedy this is to use techniques like RAG to work out what snippets of text from an external source are most similar to the users question and feeding them to the LLM along with the users question. The LLM can use this data to generate a natural sounding answer.\n",
        "\n",
        "4. Chunks:\n",
        "* A chunk is simply a smaller piece of a larger piece of text. The reason you'd want to break down, say, a book or a pdf is to find relevant pieces of information in it. After all, we don't want to throw an entire PDF at our LLM and have it decipher all of it, now do we?\n",
        "\n",
        "* We'll use a bunch of functions that we'll get to later to work out which chunks are the most relevant to our question and pass just those to our LLM.\n",
        "\n",
        "* The size of a chunk is entirely up to the programmer, but you want a chunk to be big enough for it to actually have some sort of meaning in it on its own, while not being big enought to contain too many different kinds of information in it.\n",
        "\n",
        "\n",
        "# üìå What You'll Do:\n",
        "\n",
        "\n",
        "\n",
        "1.   Create a rudimentary vector database by defining the functions in the template class below\n",
        "2.   Convert a pdf of a stock trading tutorial to a bunch of small strings (called chunks. we'll get to why later).\n",
        "3.   Use this vector database to store the embeddings of the chunks.\n",
        "4. Handle user queries by querying our database for the most relevant chunks and feeding them to our LLM to generate an output."
      ],
      "metadata": {
        "id": "xeub4nSHjoq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library installations. we'll be using sentence_transformers for our embedding model and pypdf to read a pdf of our choosing.\n",
        "\n",
        "!pip install pypdf\n",
        "!pip install sentence_transformers\n",
        "\n",
        "#fun fact: the ! is used to signify that these are shell commands"
      ],
      "metadata": {
        "id": "0aVeoPaRrttW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765fde71-ebbf-4cfa-a4d3-dc2c42c40a70"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cmtAzj17opiL"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader as PDFReader\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "#I highly recommend sticking to this LLM; its small and runs decent in a colab notebook, especially if you have GPU acceleration enabled.\n",
        "LLM_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "#Same goes for our embedding model.\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L12-v2\"\n",
        "HF_API_KEY = \"HF\"#Not a necessity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=HF_API_KEY)"
      ],
      "metadata": {
        "id": "fP3kPw_7Aq-T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "bfbca892-07b0-4469-d389-f1c8ddc5418b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "Invalid user token.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m   1736\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1737\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2 (Request ID: Root=1-67f48c32-12d4eb071a31d7084895aba0;0e4e4919-1437-429d-9fed-4d3f88a24bfc)\n\nInvalid credentials in Authorization header",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-203a4302d7ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHF_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             args_msg = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m\"you want to set the git credential as well.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             )\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mnotebook_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(token, add_to_git_credential)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must use your personal account token, not an organization token.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mtoken_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0mpermission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accessToken\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Token is valid (permission: {permission}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m   1748\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0meffective_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_token_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" The token stored is invalid. Please run `huggingface-cli login` to update it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: Invalid user token."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorDB:\n",
        "  embedModel = None#we will be using embedModel to refer to our embedding model through out this class. Its value is initialized in the constructor\n",
        "\n",
        "  nDim = 0#number of dimensions. Remember how I told you the embedding model generates points in space?\n",
        "          #nDim will contain the number of dimensions of the space these points are generated in. It will come in handy later\n",
        "\n",
        "  _embeddings = None#This is the numpy array that will actually store our embeddings. the underscore at the start of the variable name\n",
        "                    #signifies that this a private variable; i.e it should not be directly accessed outside of this class.\n",
        "\n",
        "  _strings : list[str] = []#we will also be storing the strings that we are generating embeddings for.\n",
        "\n",
        "\n",
        "  #note: _embeddings is a numpy array. THIS IS NOT THE SAME AS A REGULAR PYTHON ARRAY.\n",
        "  #      _strings on the other hand is a regular ole Python array.\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self, model_name : str) -> None:#DO NOT CHANGE. This is the constructor funtion.\n",
        "    #we've set up a lot of the boilerplate code for you; this code instantiates all the class variables and loads the embedding model.\n",
        "    self.embedModel = SentenceTransformer(model_name)\n",
        "    self.nDim = self.embedModel.get_sentence_embedding_dimension()\n",
        "\n",
        "    self._embeddings = numpy.ndarray((0, self.nDim), dtype = numpy.float32) #sets _embeddings to an array of an array.\n",
        "    #The first number (0) represents the number of arrays that are stored in _embeddings. 0 for now as its empty.\n",
        "    #nDim refers to the number of values in each of those subarrays. We need one to represent each dimension, the same way you'd need an array with 3 values to represent a 3d point.\n",
        "\n",
        "    self._strings = []\n",
        "\n",
        "  def addToDatabase(self, input : list[str]):\n",
        "    '''\n",
        "    Your code should do two things here:\n",
        "    1. Convert the strings in the input array to embeddings and add them to the _embeddings array\n",
        "    2. Store their corresponding strings (IN THE SAME ORDER) in the _strings array\n",
        "\n",
        "    Hint: the documentation for the SentenceTransformers library can be found here: https://sbert.net/docs/package_reference/sentence_transformer/index.html\n",
        "    If you want to know which function to call for anything related to the embedding model (eg: generating embeddings) have a look at this.\n",
        "    '''\n",
        "    #Your code goes here!\n",
        "    # Convert the input strings to embeddings using the embedding model\n",
        "    new_embeddings = self.embedModel.encode(input)\n",
        "    # Append the new embeddings to the existing _embeddings array\n",
        "    self._embeddings = numpy.vstack((self._embeddings, new_embeddings))\n",
        "    # Store the corresponding strings in the _strings list in the same order\n",
        "    self._strings.extend(input)\n",
        "\n",
        "    #The pass command will cause everything within this function thats after pass to be ignored.\n",
        "    #Delete it once you start coding or keep all your code above it\n",
        "    #pass\n",
        "\n",
        "  def clearDatabase(self):\n",
        "    '''\n",
        "    This function should clear the database by emptying the _embeddings and _string arrays.\n",
        "    '''\n",
        "    #Your code goes here!\n",
        "    # Clear the _embeddings array by resetting it to an empty array with the correct shape\n",
        "    self._embeddings = numpy.ndarray((0, self.nDim), dtype=numpy.float32)\n",
        "    # Clear the _strings list\n",
        "    self._strings = []\n",
        "\n",
        "    #pass\n",
        "\n",
        "  def euclideanSim(self, x, y, dimensions: int):\n",
        "    '''\n",
        "    This function calculates how close two points are using euclidean distance.\n",
        "\n",
        "    Euclidean distance isn't anything fancy; it's the most basic method for comparing the distance between two points.\n",
        "    You may have seen it being used like this: ‚àö((x2 - x1)¬≤  +  (y2 - y1)¬≤) for measuring distances in 2-D.\n",
        "\n",
        "    Your function should do the same thing, but in nDim dimensions instead.\n",
        "\n",
        "    Keep in mind that this function is meant to return similarity i.e the opposite of distance. (This isn't mandatory; just rename the function to avoid confusion if you'd rather just have it return distances)\n",
        "    After all, if two points are close to each other, their respective texts must be similar in meaning.\n",
        "    '''\n",
        "    #Your code goes here!\n",
        "    total = 0\n",
        "    for i in range(dimensions):\n",
        "      total += numpy.square(x[i] - y[i])\n",
        "    euclidian_distance = numpy.sqrt(total)\n",
        "    return euclidian_distance\n",
        "\n",
        "\n",
        "  def search(self, input : str, n_return = 1):\n",
        "    '''\n",
        "    This is the biggest function here by far.\n",
        "    This functions job is to find the n closest to the input in our database\n",
        "\n",
        "    This will be done by generating an embedding for our input, and finding the n closest points to it and what pieces of text are associated to them.\n",
        "\n",
        "    It should return a tuple (tRText, tRSim).\n",
        "    tRText will contain the n closest pieces of text and tRSim will contain their respective similarities to the query\n",
        "    '''\n",
        "    tRText = []\n",
        "    tRSim = numpy.array([])\n",
        "\n",
        "    #Your code goes here!\n",
        "    # Generate embedding for the input query\n",
        "    sentence_embedding = self.embedModel.encode(input)\n",
        "    distances = []\n",
        "    for emb in self._embeddings:\n",
        "      dist = self.euclideanSim(sentence_embedding, emb, self.nDim)\n",
        "      distances.append(dist)\n",
        "    distances = numpy.array(distances)\n",
        "\n",
        "    sorted_distances = numpy.argsort(distances)\n",
        "    closest_indices = sorted_distances[:n_return]\n",
        "\n",
        "    tRText = [self._strings[i] for i in closest_indices]\n",
        "    tRSim = distances[closest_indices]\n",
        "\n",
        "    return(tRText, tRSim)\n"
      ],
      "metadata": {
        "id": "9bCcR4eMyg1Y"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vDB = VectorDB(model_name = EMBEDDING_MODEL)\n",
        "#this code calls the constructor and sets vDB to be an instance of our vector database.\n"
      ],
      "metadata": {
        "id": "27vH301F-7rd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunksFromText(text : str):\n",
        "  '''\n",
        "  Your goal for this function is to convert the text of a page to individual chunks and store these chunks in our vector database.\n",
        "\n",
        "  Q: What do CHUNK_SIZE and CHUNK_OVERLAP mean?\n",
        "\n",
        "  A: CHUNK_SIZE is just the number of characters we are storing in each chunk. CHUNK_OVERLAP refers to the number of characters\n",
        "  shared between sequential chunks. I.E for a chunk overlap of 50 the first chunk will store store characters 0 to 500, the second 450 to 950,\n",
        "  the third 900 to 1400 and so on. Feel free to play around with these values; I chose them at random.\n",
        "\n",
        "  Also note that chunking by character count isn't the only approach that you can use. Another method is to have each chunk be a sentence,\n",
        "  multiple sentences, or an entire paragraph. Again, feel free to experiment!\n",
        "  '''\n",
        "\n",
        "  CHUNK_SIZE = 500\n",
        "  CHUNK_OVERLAP = 50\n",
        "\n",
        "  docChunks = []\n",
        "\n",
        "  #Your code goes here!\n",
        "  start = 0\n",
        "  while start < len(text):\n",
        "      chunk = text[start:start + CHUNK_SIZE]\n",
        "      docChunks.append(chunk)\n",
        "      start += CHUNK_SIZE - CHUNK_OVERLAP\n",
        "\n",
        "  vDB.addToDatabase(input = docChunks)"
      ],
      "metadata": {
        "id": "rBisrmv2rhWt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # remove headers and footers\n",
        "    if len(lines) >= 2:\n",
        "        cleaned_lines = lines[1:-1]\n",
        "    else:\n",
        "        cleaned_lines = lines\n",
        "\n",
        "    # join remaining lines\n",
        "    cleaned_text = '\\n'.join(cleaned_lines).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "def chunksFromPDF(path = \"\", startPage = 0, endPage = None):\n",
        "  '''\n",
        "  Here's the documentation for the PDFReader library: https://pdfreader.readthedocs.io/en/latest/tutorial.html\n",
        "  Your goal for this function is to extract text from each page in the PDF and call chunksFromText on the text to convert it to chunks and store\n",
        "  it in the vector database.\n",
        "\n",
        "  Quick reference: pdf = PDFReader(path) #gets us a a PDFReader object that we can call functions on\n",
        "                   pdf.pages #big array containing all the pages\n",
        "                   page = pdf.pages[0] #gets first page\n",
        "                   text = page.extract_text() #gets text from the page\n",
        "\n",
        "  You are required to clean the text from each page, as well as decide which pages are actually relevant.\n",
        "  Examples of things you should get rid of (Not exhaustive):\n",
        "  1. Headers, footers,\n",
        "  2. Indexes,\n",
        "  3. Title page,\n",
        "  etc.\n",
        "  '''\n",
        "  #Your code goes here!\n",
        "  pdf = PDFReader(path)\n",
        "  total_pages = len(pdf.pages)\n",
        "\n",
        "  if endPage is None or endPage >= total_pages:\n",
        "    endPage = total_pages - 1\n",
        "\n",
        "  for i in range(startPage, endPage + 1):\n",
        "    page = pdf.pages[i]\n",
        "    text = page.extract_text()\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    if not cleaned_text:\n",
        "      continue\n",
        "\n",
        "    chunksFromText(cleaned_text)\n",
        "\n",
        "chunksFromPDF(path = \"./Understanding Stocks.pdf\", startPage=5 )#fill in with appropriate arguments."
      ],
      "metadata": {
        "id": "D1HNuSrLf0TJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answerBot = pipeline(\"text-generation\", model = LLM_MODEL, trust_remote_code=True) # This bit of code loads our LLM into memory; we can access it using\n",
        "                                                                                   # the answerBot object.\n",
        "                                                                                   #The model we are using is 2.2GB, so make sure you've got a decent WiFi connection."
      ],
      "metadata": {
        "id": "c3A96az1-BvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e73aef-157c-48a9-f625-03576114597a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generateAnswer(question):\n",
        "  '''\n",
        "  Documentation for the pipeline object (answerBot) we created two lines ago: https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/pipelines#transformers.TextGenerationPipeline\n",
        "\n",
        "  This is it, the final piece of the puzzle. This function will take a question, find out which chunks from our vector database\n",
        "  are most relevant to it, and ask our LLM the original question with the context of the text from the relevant chunks.\n",
        "\n",
        "  Have it return a string with the LLMs answer.\n",
        "  '''\n",
        "  #Your code goes here!\n",
        "  retrieved_texts, _ = vDB.search(question, n_return=3)\n",
        "  context = \"\\n\".join(retrieved_texts)\n",
        "  prompt = f\"Context: {context}\\nQ: {question}\\nA:\"\n",
        "\n",
        "  output = answerBot(\n",
        "    prompt,\n",
        "    max_new_tokens=300,\n",
        "    do_sample=True,\n",
        "    return_full_text=False\n",
        "  )\n",
        "\n",
        "  return(output)"
      ],
      "metadata": {
        "id": "qbZ1bXEtDe8J"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generateAnswer(\"What is a 401k?\"))"
      ],
      "metadata": {
        "id": "9wJPbBVa3pb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ebfdf9-9fcd-43ea-9091-782550360aa4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': ' A 401k is an employee-sponsored retirement plan offered by many companies. If you work at a company that offers this type of plan, it allows you to choose to get a portion of your salary deposited into a 401k account. The money in a 401k is tax-deferred, meaning it‚Äôs not taxed until you withdraw it as you retire. However, since it‚Äôs tax-free, you can also invest money in a 401k in an attempt to earn extra income and investments, even though it won‚Äôt be taxed! The beauty of using this strategy in a company-sponsored plan is that your gains are tax-free, unlike in a regular taxable savings account where your profits are taxed at your current income level on a yearly basis. The second aspect of using this strategy in company-sponsored plans is the fact that you can exchange money between different funds within your company-sponsored plan. This means that if you‚Äôre bored with one stock holding, you can exchange it for another without any penalty. This is great since this type of investment plan only lets you choose on a regular basis who to invest your money in. One of the smartest ways to invest in mutual funds is through a 401(k). When you move all of your retirement savings into'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Wrapping Up\n",
        "\n",
        "And that should be it! Try calling the generate answer function we just defined with a finance related question.\n",
        "\n",
        "I'd recommend asking it what a 401(k) is. The PDF we used uses a fancy bit of unicode to write 59.5 (which is how old you have to be before you start paying taxes on a 401k), and if our code is working properly, the LLM will work that character into its answer.\n",
        "\n",
        "I hope you had fun working on this project and learned something new"
      ],
      "metadata": {
        "id": "Yc4plZnA-2nB"
      }
    }
  ]
}